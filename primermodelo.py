# -*- coding: utf-8 -*-
"""PrimerModelo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zl1nYP8X_fVho8HsjDAjqJbY9PP7KAkV
"""

#!pip install 'neptune-contrib[monitoring]>=0.24.9'

#!pip install -U xgboost

import pandas as pd
from xgboost import XGBRegressor
import xgboost as xgb
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# here you import `neptune_calback` that does the magic (the open source magic :)
from neptunecontrib.monitoring.xgboost_monitor import neptune_callback
from sklearn import preprocessing
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score


df_Base_Datos = pd.read_csv("Archivo_Latitud_Limpia.csv", encoding = 'latin-1')
X = pd.read_csv('Archivo_Latitud_Limpia.csv',usecols = [i for i in range(17)], encoding = 'latin-1')
lbl = preprocessing.LabelEncoder()
X['Año/Mes'] = lbl.fit_transform(X['Año/Mes'].astype(str))
X['Piso'] = lbl.fit_transform(X['Piso'].astype(str))
X['Categoría del bien'] = lbl.fit_transform(X['Categoría del bien'].astype(str))
X['Estado de conservación'] = lbl.fit_transform(X['Estado de conservación'].astype(str))
X['Método Representado'] = lbl.fit_transform(X['Método Representado'].astype(str))
X['Área Terreno'] = lbl.fit_transform(X['Área Terreno'].astype(str))
X['Área Construcción'] = lbl.fit_transform(X['Área Construcción'].astype(str))
#Y['Año/Mes'] = lbl.fit_transform(X['Año/Mes'].astype(str))

y = pd.read_csv('Archivo_Latitud_Limpia.csv', usecols=["Valor comercial (USD)"], encoding = 'latin-1')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(y_train.dtypes)

model = XGBRegressor(n_estimators=1000, max_depth=100, eta=0.7, subsample=0.7, colsample_bytree=0.8)

model.fit(X_train,y_train)

y_pred = model.predict(X_test)
print(mean_absolute_error(y_test, y_pred))
print(mean_squared_error(y_test, y_pred))
print(r2_score(y_test, y_pred))

