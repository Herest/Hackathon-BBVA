# -*- coding: utf-8 -*-
"""PrimerModelo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zl1nYP8X_fVho8HsjDAjqJbY9PP7KAkV
"""

#!pip install 'neptune-contrib[monitoring]>=0.24.9'

#!pip install -U xgboost

import pandas as pd
from xgboost import XGBRegressor
import xgboost as xgb
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

# here you import `neptune_calback` that does the magic (the open source magic :)
from neptunecontrib.monitoring.xgboost_monitor import neptune_callback
from sklearn import preprocessing
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score


df_Base_Datos = pd.read_csv("Archivo_Latitud_Limpia.csv", encoding = 'latin-1')
X = pd.read_csv('Archivo_Latitud_Limpia.csv',usecols = [i for i in range(17)], encoding = 'latin-1')
for col in ['VIA','Categoría del bien','Estado de conservación','Método Representado']:
    X=pd.concat([X,pd.get_dummies(X[col])],axis=1)
    X.drop(col,axis=1,inplace=True)


y = pd.read_csv('Archivo_Latitud_Limpia.csv', usecols=["Valor comercial (USD)"], encoding = 'latin-1')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = XGBRegressor(n_estimators=1000)

model.fit(X_train,y_train)

y_pred = model.predict(X_test)
print(mean_absolute_error(y_test, y_pred))
print(mean_squared_error(y_test, y_pred))
print(r2_score(y_test, y_pred))


from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold

model = XGBRegressor(n_estimators=2000)
cv = RepeatedKFold(n_splits=10, n_repeats=10)
# evaluate model
scores = cross_val_score(model, X, y, scoring='r2', cv=cv)

from sklearn.ensemble import RandomForestRegressor

regrs=RandomForestRegressor(n_estimators=1000,)